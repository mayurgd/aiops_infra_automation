create_repo_task:
  description: >
    Create a GitHub repository with the following specifications:
    - use_case_name: {use_case_name}
    - template: {template}
    - internal_team: {internal_team}
    - development_team: {development_team}
    - additional_team: {additional_team}

    Use the create_github_repository tool to execute your tasks.
    Monitor the creation process and report the final status.
    If Inputs are invalid just respond to the user to provide correct inputs.
  expected_output: >
    A detailed report of the repository creation process including:
    - Validation results
    - Creation status
    - Repository URL (if successful)
    - Any errors encountered
    - Next steps for the user
  agent: git_repo_agent

create_databricks_schema_task:
  description: >
    Create a Databricks catalog and schema with the following specifications:
    - catalog: {catalog}
    - schema: {schema}
    - aiml_support_team: {aiml_support_team}
    - aiml_use_case: {aiml_use_case}
    - business_owner: {business_owner}
    - internal_entra_id_group: {internal_entra_id_group}
    - external_entra_id_group: {external_entra_id_group}

    Use the create_databricks_schema tool to execute your tasks. 
    Ensure proper permissions and access controls are set up.
    If Inputs are invalid just respond to the user to provide correct inputs.
  expected_output: >
    A comprehensive report of the Databricks schema creation process including:
    - Input validation results
    - Schema creation status
    - Catalog and schema details (if successful)
    - Permission setup status
    - Any errors encountered
    - Access instructions for users
  agent: databricks_schema_agent

create_databricks_compute_task:
  description: >
    Create a Databricks compute cluster with the following specifications:
    - cluster_name: {cluster_name}
    - spark_version: {spark_version}
    - driver_node_type_id: {driver_node_type_id}
    - node_type_id: {node_type_id}
    - min_workers: {min_workers}
    - max_workers: {max_workers}
    - data_security_mode: {data_security_mode}
    - aiml_use_case: {aiml_use_case}

    Use the create_databricks_compute tool to execute your tasks.
    Ensure proper cluster configuration and resource allocation.
    If Inputs are invalid just respond to the user to provide correct inputs.
  expected_output: >
    A comprehensive report of the Databricks compute cluster creation process including:
    - Input validation results
    - Cluster creation status
    - Cluster configuration details (if successful)
    - Resource allocation status
    - Any errors encountered
    - Access instructions for users
  agent: databricks_compute_agent

supervision_task:
  description: >
    Analyze the user request and available inputs to determine the appropriate automation workflow. 
    Based on the user query: "{user_query}" and available input parameters, decide whether to:
    
    PHASE 1 - DETERMINE REQUIREMENTS:
    - Ask the user what they want to create:
      1. GitHub repository
      2. Databricks Schema 
      3. Databricks Compute
      4. Multiple resources (specify which ones)
    
    PHASE 2 - COLLECT INPUTS:
    Once user confirms their choice, collect ALL required inputs for their selected resources.
    Show the user what the required inputs are, they can either give all information at once
    or give it one by one, assess and act accordingly
    Use the get_human_input tool to gather information step by step.
    
    Required inputs by resource type:
    
    **GitHub Repository:**
    - use_case_name: Name for the repo
    - template: Repository template to use
    - internal_team: Internal team name
    - development_team: Development team name  
    - additional_team: Any additional team
    
    **Databricks Schema:**
    - catalog: Databricks catalog name
    - schema: Schema name within the catalog
    - aiml_support_team: AI/ML support team
    - aiml_use_case: usecase name to create a tag
    - business_owner: Business owner contact
    - internal_entra_id_group: Internal Azure AD group
    - external_entra_id_group: External Azure AD group
    
    **Databricks Compute:**
    - cluster_name: Name for the compute cluster
    - spark_version: Apache Spark version
    - driver_node_type_id: Driver node instance type
    - node_type_id: Worker node instance type
    - min_workers: Minimum number of worker nodes
    - max_workers: Maximum number of worker nodes
    - data_security_mode: Security mode for data access
    - aiml_use_case: usecase name to create a tag, default to cluster name if not provided
    
    PHASE 3 - CONFIRMATION:
      Once ALL inputs are collected, present a clear summary of what will be created
      Ask for explicit confirmation: 
      """
      Do you want to proceed with creating these resources with the above configuration? Please confirm 'yes' to proceed or 'no' to modify.
      ### Summary of Resources to Create:
      [List each resource type and all collected parameters in a clear format]
      """
    
    PHASE 4 - EXECUTION:
      ONLY after receiving explicit 'yes' confirmation, delegate to the appropriate specialized agents.
      DO NOT trigger any tools or delegate tasks until explicit confirmation is received.
    
    IMPORTANT RULES:
    - DO NOT create any resources until the user explicitly confirms with 'yes'
    - Always validate that ALL required inputs are provided before asking for confirmation
    - Use clear markdown formatting in your questions for better readability
    - If user says 'no' or wants to modify, go back to input collection phase

  expected_output: >
    A comprehensive workflow that includes:
    - Clear determination of user requirements
    - Complete collection of all required inputs using structured questions
    - A formatted summary of all collected information
    - Explicit user confirmation before proceeding
    - Only after confirmation: delegation to appropriate specialized agents
    - Clear status updates throughout the process

  agent: supervisor_agent

  additional_requirements:
    - present_inputs_structured: >
        Once all the inputs have been gathered while confirmation present all the 
        gathered inputs in structured format so that user can see the values in 
        one place to provide a confirmation